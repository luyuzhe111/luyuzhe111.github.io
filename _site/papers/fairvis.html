<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <link href="https://fonts.googleapis.com/css?family=Gaegu:300,700" rel="stylesheet">

  <!-- Enable responsiveness on mobile devices -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Share card -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@yuzhe_lu" />
  <meta name="twitter:creator" content="@yuzhe_lu" />
  <meta property="og:url" content="https://luyuzhe111.github.io/" />
  <meta property="og:title" content="Bryan Lu" />
  <meta property="og:description" content="Bryan Lu is a graduate student in MLD at CMU." />
  <meta property="og:image" content="https://luyuzhe111.github.io/images/share.png" />

  <title>
    
      Fairvis — Yuzhe (Bryan) Lu | Academic Website
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/styles.css">
  <!-- <link rel="stylesheet" href="styles.css"> -->
  <link href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png?v=xQdLjRyXLj">
  <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png?v=xQdLjRyXLj">
  <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png?v=xQdLjRyXLj">
  <link rel="manifest" href="/icons/site.webmanifest?v=xQdLjRyXLj">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg?v=xQdLjRyXLj" color="#313131">
  <link rel="shortcut icon" href="/icons/favicon.ico?v=xQdLjRyXLj">
  <meta name="msapplication-TileColor" content="#313131">
  <meta name="msapplication-config" content="/icons/browserconfig.xml?v=xQdLjRyXLj">
  <meta name="theme-color" content="#ffffff">

  <!-- Feed -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Yuzhe (Bryan) Lu | Academic Website" />

  <!-- Google Analytics -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-180647955-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-180647955-1');
  </script>



</head>

  <body>
  
    <header class="masthead">
	
	<h1 class="masthead-title">
  		<a href="/" title="Home">Yuzhe (Bryan) Lu | Academic Website</a>
  		<div class="mastheadspacer"></div>
   		<span class="masthead-slash">/</span>
   		<small></small>
	</h1>
</header>

    <nav>
	<div class="nav">
	
	</div>
</nav>

    <div class="container content">

      <main>
        <article class="post paper">
  
<p>#</p>
<p>

</p>

<figure>
    <img class="single" src="/images/papers/19-fairvis-vast.png" />
    <figcaption class="single">
      FairVis integrates multiple coordinated views for discovering intersectional bias. 
Above, our user investigates the intersectional subgroups of <i>sex</i> and <i>race</i>. 
A. The Feature Distribution View allows users to visualize each feature's distribution and generate subgroups.
B. The Subgroup Overview lets users select various fairness metrics to see the global average per metric and compare subgroups to one another, e.g., pinned Caucasian Males versus hovered African-American Males.
The plots for <i>Recall</i> and <i>False Positive Rate</i> show that for African-American Males, the model has relatively high recall but also the highest false positive rate out of all subgroups of sex and race. 
C. The Detailed Comparison View lets users compare the details of two groups and investigate their class balances.
Since the difference in False Positive Rates between Caucasian Males and African-American Males is far larger than their difference in base rates, a user suspects this part of the model merits further inquiry. 
D. The Suggested and Similar Subgroup View shows suggested subgroups ranked by the worst performance in a given metric. 


    </figcaption>
</figure>

<h2 id="abstract">Abstract</h2>
<p>The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people.
Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups.
Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups.
We present FairVis, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models.
Through FairVis, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups.
FairVis’ coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups.
We show how FairVis helps to discover biases in two real datasets used in predicting income and recidivism.
As a visual analytics system devoted to discovering bias in machine learning, FairVis demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.</p>

<h2 id="citation">Citation</h2>
<p>

	<strong></strong>


<br />



<br />



<i>.  .</i>

<br />





</p>

<!-- ## BibTeX
```


``` -->

</article>

      </main>

    </div>

    <footer>
	<div id="footer-left">
		<a href="mailto:yuzhe.lu@vanderbilt.edu"><i class="fa-lg fa fa-envelope footer-icon" aria-hidden="true"></i></a>
		<a href="https://twitter.com/yuzhe_lu"><i class="fa-lg fab fa-twitter footer-icon" aria-hidden="true"></i></a>
		<a href="https://www.linkedin.com/in/bryan-lu-419623180/"><i class="fa-lg fab fa-linkedin-in footer-icon" aria-hidden="true"></i></a>
		<a href="https://github.com/luyuzhe111"><i class="fa-lg fab fa-github footer-icon" aria-hidden="true"></i></a>
		<a href="https://scholar.google.com/citations?user=R6bq6u4AAAAJ&hl=en"><i class="fa-lg fa fa-graduation-cap footer-icon" aria-hidden="true"></i></a>
		<br>
		<br>
		&copy; <time datetime="September 4, 2022">2022</time> Yuzhe Lu. Design inspiration from <a style="color:#e6e6e6" href="https://willepperson.com/">willepperson.com</a>
	</div>
	
	<div id="footer-right">
	</div>
</footer>




  </body>
</html>
